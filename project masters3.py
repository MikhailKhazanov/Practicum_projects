#!/usr/bin/env python
# coding: utf-8

# ## <span style="color:magenta">Привет, Михаил!</span>
# 
# 
# Меня зовут **Саша Сушков** и я буду проверять твой проект. Если не против, я буду обращаться к тебе на "ТЫ". Если это неприемлемо, обязательно напиши мне в комментариях - мы прейдем на "ВЫ".
# 
# Спасибо, что сдал задание:) Ты проделал отличную работу. Далее в файле ты сможешь найти мои комментарии в подобных ячейках. Пострайся учесть эти комментарии в ходе выполнения дальнейших проектов.
# 
# Мне очень понравился твой проект. Видно, что ты достаточно проработал теоритическую часть, даже при не таком большом объёме проекта. Это очень здОрово! Тем не менее, в проекте обнаружились недочёты, которые необходимо будет исправить. Это не плохо, это - нормально.
# 
# Обрати внимание, для твоего удобства, я буду выделять свои комментарии цветом, в зависимости от того, какие действия, важность и особенности тебя ждут в процессе проверки:
# <div class="alert alert-block alert-success">
# <h2>Комментарий от ревьювера (все здОрово)</h2> 
#     
# Такими комментариями я буду помечать хорошо проделаную работу, а так же хорошо проделаную работу над ошибками:)
# </div>
# 
# <div class="alert alert-block alert-warning">
# <h2>Комментарий от ревьювера (принимаем к размышлению)</h2> 
#     
# В таких комментариях я постараюсь подсказать тебе более элегантное или легкое решение. Поскажу некоторые хитрости и фишки. Дам рекомендации на будущее, ссылки для доп. изучения. Части проекта, помеченные такими комментариями, можно не исправлять, но рекомендую обратить на них внимание.
# </div>
# 
# <div class="alert alert-block alert-danger">
# <h2>Комментарий от ревьювера (недочет, который нужно доработать)</h2> 
#     
# Если ты видишь такой комментарий, значит я указываю тебе на недочёт, его нужно исправить. Без такого исправления я не приму твою работу:)
# </div>
# 
# 
# 
# #### Пожалуста, не удаляй мои комментарии и относись к ним бережно. При повторной проверке, я буду ориентироваться на них. 
# 
# ###### Можешь под моими комментариями оставлять свои, в которых опиши свои испарвления. Это облегчит процесс проверки и мне, и тебе:) Например, вот так:
# 
# <div class="alert alert-block alert-info">
# <b>КОММЕНТАРИЙ ОТ УЧЕНИКА:</b>
# </div>

# <div class="alert alert-block alert-info">
# <b>КОММЕНТАРИЙ ОТ УЧЕНИКА: Саша, привет! Ну конечно, мы тут со всеми на "ты", так проще) Спасибо тебе большое за такое подробное ревью! Действительно, надо все делать лучше, благодаря советам будем стараться) У тебя отличные позитивные методы мотивации, настоящий Лид с большой буквы! 
#     
# Надеюсь, что все мои изменения сохранятся, тетрадка "глючит" периодически при отправке.
# </div>

# # Итоговый проект от Мастерской
# 
# 
# - Автор: Хазанов Михаил
# - Дата: 04.11.2025

# <div class="alert alert-block alert-info">
# <b>Совет: </b> Саша, привет! У меня ревьюеры в некоторых проектах при отправке тетрадки возвращали задание, якобы не все отправилось. Если вдруг что-то не хватает, не засчитывай это как попытку, пожалуйста.
# </div>

# <div class="alert alert-block alert-success">
# <h2>Комментарий от ревьювера (все здОрово)</h2> 
#     
# Все норм!
# </div>

# ## Описание проекта
# 
# Вы работаете аналитиком данных в Яндекс Такси. Каждый день компания обрабатывает миллионы поездок, оплаченных разными способами. Чтобы финансовая и продуктовая команды регулярно получали актуальные данные о выручке и поведении пассажиров, коллеги решили настроить автоматическую обработку этих данных.
# 
# Ваша задача — построить витрину данных, которая будет агрегировать информацию о поездках по каждому способу оплаты. Для этого нужно написать PySpark-скрипт, который рассчитает ключевые показатели: количество поездок, среднюю стоимость поездки, средние чаевые и суммарную выручку по каждому типу оплаты.
# 
# Чтобы процесс был полностью автоматическим и не зависел от ручных запусков, необходимо создать DAG в Airflow. Этот DAG должен ежедневно:
# 
# * проверять наличие новых файлов с данными;
# 
# * запускать Spark-задачу;
# 
# * формировать обновлённую итоговую таблицу.
# 
# Эта таблица станет основой для финансовых отчётов и аналитических дашбордов.

# ## Описание данных
# 
# Таблица `taxi_data` содержит данные об активности пользователей и состоит из следующих полей:
# 
# * `taxi_id` — идентификатор водителя;
# 
# * `trip_start_timestamp` — время начала поездки;
# 
# * `trip_end_timestamp` — время окончания поездки;
# 
# * `trip_seconds` — длительность поездки в секундах;
# 
# * `trip_miles` — дистанция поездки;
# 
# * `fare` — стоимость поездки;
# 
# * `tips` — размер чаевых;
# 
# * `trip_total` — общая стоимость поездки: стоимость поездки + чаевые + комиссия;
# 
# * `payment_type` — способ оплаты.

# ## Что нужно сделать
# 
# В проекте вам предстоит автоматизировать подготовку витрины данных по поездкам Яндекс Такси:
# 
# 1. Сначала необходимо написать Spark-скрипт, который будет обрабатывать данные о поездках и агрегировать показатели по способам оплаты `payment_type`. Понадобится рассчитать несколько показателей:
# 
# * количество поездок, которое показывает общий спрос и загрузку сервиса;
# * среднюю стоимость `fare`, которое отражает уровень среднего чека поездки;
# * средние чаевые `tips` — индикатор удовлетворённости клиентов и мотивации водителей;
# * суммарную выручку `trip_total` — ключевой показатель дохода компании.
# 
# Все результаты должны собираться в одну итоговую таблицу `taxi_payment_summary`. После этого таблицу нужно записать в ClickHouse с помощью JDBC-драйвера.
# 
# 2. Далее вам понадобится настроить DAG в Airflow. DAG должен запускаться ежедневно. Перед запуском он проверяет наличие файла с данными за нужную дату в S3-хранилище и только после появления файла запускает Spark-задачу.
# 
# Когда всё будет готово, перенесите свой код и результаты в шаблон для ревьюеров в следующем уроке. Учтите, что этот шаблон служит только для передачи решения — запустить в нём код не получится.

# ## Шаг 1. Настройте Spark-агрегацию

# Ваши данные для подключения к DBeaver:
# *   Имя пользователя — da_20250920_ee042b41e4
# *   Пароль — 65d42ceb209041d3b333c67ed7c136a1

# Данные хранятся в формате Parquet, поэтому для чтения используйте метод `spark.read.parquet()`. Это быстрее и надёжнее, чем CSV.
# 
# Сгруппируйте данные по полю `payment_type` и рассчитайте четыре показателя:
# 
# * количество поездок — `count(*)`;
# * среднюю стоимость — `avg(...)`;
# * средние чаевые — `avg(...)`;
# * суммарную выручку — `sum(...)`.
# 
# Так получится витрина для анализа информации по каждому способу оплаты. После этого настройте запись полученной таблицы в ClickHouse. Проверьте, что всё работает, и переходите в шагу 2.

# <div class="alert alert-block alert-success">
# <h2>Комментарий от ревьювера (все здОрово)</h2> 
#     
# Хорошо, что оставляешь контекст задания, это может показаться не сильно полезным в проекте, но в реальной работе очень важно использовать комментарии и пояснения.
# </div>
# filename=my_spark_job.py
from pyspark.sql import SparkSession
import pyspark.sql.functions as F# Создаем Спарк-сессию
spark = SparkSession.builder.appName("taxi_payment_analysis").config("fs.s3a.endpoint", "storage.yandexcloud.net").getOrCreate()# Параметры подключения к ClickHouse
jdbcPort = 8443
jdbcHostname = "rc1a-3jouval14nne7aun.mdb.yandexcloud.net"
username = "da_20250920_ee042b41e4"
jdbcDatabase = "playground_da_20250920_ee042b41e4"
jdbcUrl = f"jdbc:clickhouse://{jdbcHostname}:{jdbcPort}/{jdbcDatabase}?ssl=true"
password = "65d42ceb209041d3b333c67ed7c136a1"# Чтение входных данных из S3 (Parquet)
df = spark.read.parquet(f"s3a://da-plus-dags/project_04/taxi_data.parquet")# Агрегация данных
result_df = df.groupBy("payment_type").agg(
    F.count("*").alias("trip_count"),
    F.avg("fare").alias("avg_fare"),
    F.avg("tips").alias("avg_tips"),
    F.sum("trip_total").alias("total_revenue")
)# Делаем запись в ClickHouse
result_df.write.format("jdbc") \
    .option("url", jdbcUrl) \
    .option("user", username) \
    .option("password", password) \
    .option("dbtable", "taxi_payment_summary") \
    .option("createTableOptions", "ENGINE=MergeTree() ORDER BY payment_type") \
    .mode("overwrite") \
    .save()
spark.stop()
# <div class="alert alert-block alert-warning">
# <h2>Комментарий от ревьювера (принимаем к размышлению)</h2> 
#     
#     Отличная часть, но нам нужно довести её до идеала!
#     
# 1. В параметрах логин и пароль лучше сделать через переменные, и загружать в файл будем через эти переменные. Лучше не хардкорить в последнем пункте, указывая данные напрямую.
#     
# 2. Я рекомендую использовать больше комментариев, у тебя они отсутствуют полностью. Если ты заполняешь параметры, агрегаты, лучше подписвать шаги почаще. Не прям каждую строку, но для понимания, что за что отвечает, где какой параметр. Это будет полезно как для тебя (вернуться в свой скрипт через пол года и понять, что ничего не помнишь - такое себе), так и для твоих коллег, которые возможно будут смотреть код.
# 
# 3. Старайсе размещать код в ячейках для кода. Так он удобнее и интуитивнее читаеться, потому что подсвечивается.
#     
# 4. Можно вместо append успользовать overwrite. Для того, чтобы у нас каждый раз не новый объект создавался а перезаписывался) Павда придётся добавить параметр .option("createTableOptions", "ENGINE=MergeTree() ORDER BY payment_type") \
# </div>

# <div class="alert alert-block alert-info">
# <b>КОММЕНТАРИЙ ОТ УЧЕНИКА: Поправил "хардкор"))), однако код сохранил в RAW. КОмментарии внес, Overwrite добавил с боем)
# </div>

# ## Шаг 2. Настройте DAG
# 
# Дату в качестве параметра передавать не нужно. В DAG используйте `S3KeySensor`, чтобы дождаться появления файла в S3. После этого запускайте `DataprocCreatePysparkJobOperator`, передав путь к вашему скрипту. Дополнительный класс-оператор создавать не требуется.

# Ваши данные для подключения к Airflow:
# *   IP — 51.250.9.30
# *   Имя пользователя — da_20250920_ee042b41e4
# *   Пароль — 65d42ceb209041d3b333c67ed7c136a1
#filename=taxi_dag.py
from datetime import datetime
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.yandex.operators.dataproc import DataprocCreatePysparkJobOperator

DAG_ID = "taxi_payment_final"

# Создаем DAG
with DAG(
    dag_id=DAG_ID,
    start_date=datetime(2025, 1, 1),
    schedule_interval="0 16 * * *", # ежедневно в 16 часов
    catchup=False,
    tags=["taxi", "payment_final"]
) as dag:
    # Ожидаем файл с данными
    wait_for_input = S3KeySensor(
        task_id="wait_for_input_file",
        bucket_name="da-plus-dags",
        bucket_key="project_04/taxi_data.parquet",
        aws_conn_id="s3",
        poke_interval=300,
        timeout=3600,
        mode="poke",
        wildcard_match=False
    )
    # Запуск Sparkjob
    run_pyspark = DataprocCreatePysparkJobOperator(
        task_id="run_pyspark_job",
        main_python_file_uri="s3a://da-plus-dags/da_20250920_ee042b41e4/jobs/my_spark_job.py",
        cluster_id="c9q4134h5vi546h1e148",
        args=[],
        dag=dag
    )
    
    # Определяем порядок выполнения
    wait_for_input >> run_pyspark
# <div class="alert alert-block alert-danger">
# <h2>Комментарий от ревьювера (недочет, который нужно доработать)</h2> 
#     
# Отличная часть, вопросов нет, но нужно дописть зависимось между задачими. Одну строку в конце;)
# </div>

# <div class="alert alert-block alert-info">
# <b>КОММЕНТАРИЙ ОТ УЧЕНИКА: Я совсем забыл про wait_for_input >> run_pyspark ...
#     
# </div>

# <div class="alert alert-block alert-warning">
# <h2>Комментарий от ревьювера (принимаем к размышлению)</h2> 
#     
# Комментариев маловато! Тут уже помечу жёлтым цветом, но стоит в пути main_python_file_uri тоже пользователя прописать через переменную. А ещё расписание можно прописывать через schedule='0 16 * * *'. Это пример, он обозначает, что запуск будет происходить 
#     Значение:
# 
# 0 → запуск в 0-й минуте,
# 
# 16 → в 16 часов (т.е. в 16:00),
# 
#  (*) → каждый день,
# 
#  (*) → каждый месяц,
# 
#  (*) → любой день недели.
# </div>

# ![image.png](attachment:image.png)
#       

# ![image.png](attachment:image.png)

# ![image-2.png](attachment:image-2.png)

# <div class="alert alert-block alert-info">
# <b>КОММЕНТАРИЙ ОТ УЧЕНИКА: Стало лучше)
#     
# </div>

# В Кликхаусе сначала код:
# 
# CREATE TABLE IF NOT EXISTS taxi_payment_summary (
#     payment_type String,
#     trip_count UInt64,
#     avg_fare Float64,
#     avg_tips Float64,
#     total_revenue Float64
# )
# ENGINE = MergeTree()
# ORDER BY payment_type;

# Итог в кликхаусе, выбирал DISTINCT payment_type, иначе много дубликатов, часто запускал ДАГ:
# 
# SELECT DISTINCT payment_type, trip_count, avg_fare, avg_tips, total_revenue     
# FROM taxi_payment_summary
# ORDER BY payment_type; 

# <div class="alert alert-block alert-success">
# <h2>Комментарий от ревьювера (все здОрово)</h2> 
#     
# Спасибо за такой подробный разбор, это очень помогает. Как побороть дубликаты я описал в части с оверврайтом)
# </div>

# ## Шаг 3. Запустите DAG с помощью Airflow UI
# 
# Теперь можно переходить к запуску. Нажмите кнопку «Проверить», подождите 5 минут и снова нажмите её. Вам будут показаны данные для входа в веб-интерфейс Airflow. В интерфейсе найдите ваш DAG и запустите его.
# 
# Проверьте, что DAG выполнился, а результат соответствует ожиданиям. Если всё получилось — поздравляем, проект завершён!

# ## <span style="color:magenta">В заключении)</span>
# 
# Отличная работа, мне она очень нравиться!
# 
# Короче, молодец! Жду твоих исправлений) Одно маленькое и работа будет принята.
# 
# <img src="https://avatars.mds.yandex.net/get-pdb/2402172/12f53009-3c0e-4655-87c6-606d139bbf8e/s1200?webp=false" width="300">

# <div class="alert alert-block alert-info">
# <b>КОММЕНТАРИЙ ОТ УЧЕНИКА: Саш, спасибо еще раз за подробное ревью! Желаю море удачи и дачу у моря))) Отличное ревью и отличная мотивация, чтобы стать лучше! Котэ красавчик)
#     
# </div>

# In[ ]:




